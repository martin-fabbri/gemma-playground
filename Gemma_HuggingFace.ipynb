{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install git+https://github.com/huggingface/transformers\n",
    "#!pip install -q datasets loralib sentencepiece\n",
    "!pip -qq install bitsandbytes accelerate xformers einops\n",
    "!pip -qq install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 22 19:34:01 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   27C    P0    61W / 400W |   8789MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma - 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92ee74a64614caa80a6f8a86043b6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import interpreter_login\n",
    "\n",
    "# interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c34b562419b4195aa4fea9be507f88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24c2695b1f04b6a9ded9ed738aef64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", # 'google/gemma-2b-it\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map=\"auto\"\n",
    "                                             )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                         max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gemma - 7B 4Bit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "user\n",
       "What is the difference between LLaMAs, Alpacas, and Vicunas\n",
       "model\n",
       "Sure, here is the difference between LLamas, Alpacas, and Vicunas:\n",
       "\n",
       "**LLamas:**\n",
       "\n",
       "* **Origin:** North America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** Gray, brown, black, white\n",
       "* **Temperament:** Friendly, calm, and easy to train\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "**Alpacas:**\n",
       "\n",
       "* **Origin:** South America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** White, fawn, gray, brown\n",
       "* **Temperament:** Friendly, calm, and intelligent\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "**Vicunas:**\n",
       "\n",
       "* **Origin:** South America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** White, fawn, gray, brown\n",
       "* **Temperament:** Friendly, calm, and intelligent\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "The main differences between LLamas, Alpacas, and Vicunas are their origin, coat color, temperament, and uses. LLamas are North American animals, while Alpacas and Vicunas are South American animals. LLamas have a wide range of coat colors, while Alpacas and Vicunas have a more limited range of coat colors. LLamas are generally friendly and easy to train, while Alpacas and Vicunas are also friendly and easy to train. LLamas are primarily used for wool production and meat production, while Alpacas and Vicunas are primarily used for wool production and meat production."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"## Gemma - 7B 4Bit\"))\n",
    "\n",
    "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# text = text.replace(prompt, '', 1)\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90):\n",
    "    lines = text.split(\"\\n\")\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = \"\\n\".join(wrapped_lines)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, system_prompt=\"\", max_length=512):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = system_prompt\n",
    "    else:\n",
    "        system_prompt = \"You are a friendly and helpful assistant\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{system_prompt} \\n\\n {input_text}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids = inputs.to(model.device),\n",
    "        max_new_tokens = max_length,\n",
    "        do_sample = True,\n",
    "        temperature = 0.1,\n",
    "        top_k = 50,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(\n",
    "        outputs[0], \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    text = text.replace(f\"user\\n {system_prompt} \\n\\n {input_text} \\n model\", \"\", 1)\n",
    "    wrappped_text = wrap_text(text)\n",
    "    display(Markdown(wrappped_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "You are Gemma, a large language model trained by Google. Write out your reasoning step-by-\n",
       "step to be sure you get the right answers!\n",
       "\n",
       " Write a detailed analogy between mathematics and a lighthouse.\n",
       "model\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the key similarities between mathematics and a lighthouse.**\n",
       "\n",
       "* **Mathematics** is a systematic body of knowledge that deals with numbers, shapes,\n",
       "space, and other abstract concepts.\n",
       "* **Lighthouse** is a tall structure that is used to guide ships at night.\n",
       "\n",
       "**Step 2: Identify the key differences between mathematics and a lighthouse.**\n",
       "\n",
       "* **Mathematics** is an abstract discipline, while a lighthouse is a physical structure.\n",
       "* **Mathematics** is a theoretical field, while a lighthouse is a practical application of\n",
       "mathematics.\n",
       "\n",
       "**Step 3: Draw the analogy.**\n",
       "\n",
       "Mathematics is like a lighthouse because it is a system of knowledge that provides\n",
       "guidance and direction. Just as a lighthouse is a beacon that guides ships, mathematics is\n",
       "a beacon that guides people in understanding the world.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The analogy between mathematics and a lighthouse is appropriate because both are systems\n",
       "of knowledge and guidance. Mathematics is a vast and complex field of study that provides\n",
       "a framework for understanding and solving a wide range of problems. Similarly, a\n",
       "lighthouse is a tall structure that provides a clear and visible guide for ships."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate(\n",
    "    \"Write a detailed analogy between mathematics and a lighthouse.\",\n",
    "    system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "You are Gemma, a large language model trained by Google. Write out your reasoning step-by-\n",
       "step to be sure you get the right answers!\n",
       "\n",
       " Write a detailed analogy between mathematics and a music.\n",
       "model\n",
       "**Reasoning:**\n",
       "\n",
       "**Step 1: Identify the similarities between mathematics and music.**\n",
       "\n",
       "* **Structure:** Mathematics and music both have a structured framework. In mathematics,\n",
       "there are rules for arithmetic, geometry, and algebra. In music, there are rules for\n",
       "pitch, rhythm, and melody.\n",
       "* **Elements:** Both disciplines have elements that are essential to their respective\n",
       "fields. In mathematics, there are numbers, variables, and formulas. In music, there are\n",
       "notes, chords, and rhythms.\n",
       "* **Expression:** Mathematics and music are both expressive mediums. In mathematics,\n",
       "formulas can be used to express complex ideas. In music, composers use notes and rhythms\n",
       "to express emotions and stories.\n",
       "\n",
       "**Step 2: Identify the differences between mathematics and music.**\n",
       "\n",
       "* **Purpose:** Mathematics is primarily used for problem-solving and analysis. Music is\n",
       "primarily used for entertainment and emotional expression.\n",
       "* **Language:** Mathematics uses a precise language of symbols and notation. Music uses a\n",
       "language of notes, chords, and rhythms.\n",
       "* **Performance:** Mathematics is typically performed in a theoretical setting. Music is\n",
       "performed live, in front of an audience.\n",
       "\n",
       "**Step 3: Draw the analogy.**\n",
       "\n",
       "Mathematics and music are two different disciplines that share some similarities and\n",
       "differences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 126 ms, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a detailed analogy between mathematics and a music.',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "Write out your reasoning step-by-step to be sure you get the right answers!\n",
       "\n",
       " What is the difference between a Llama, Vicuna and an Alpaca?\n",
       "model\n",
       "**Reasoning:**\n",
       "\n",
       "**1. Identify the characteristics of each animal:**\n",
       "\n",
       "- **Llama:**\n",
       "   - Thick fur, long legs, docile temperament, and a distinctive hum.\n",
       "- **Vicuna:**\n",
       "   - Soft, lustrous fleece, fine wool, small stature, and a gentle temperament.\n",
       "- **Alpaca:**\n",
       "   - Dense fleece, soft wool, medium stature, and a calm temperament.\n",
       "\n",
       "**2. Compare the characteristics:**\n",
       "\n",
       "- **Llama:** Known for its thick fur and docile temperament, which are not necessarily\n",
       "desirable for breeding purposes.\n",
       "- **Vicuna:** Distinguished by its soft, lustrous fleece and fine wool, making it highly\n",
       "sought-after for its exceptional quality.\n",
       "- **Alpaca:** Distinguished by its dense fleece, soft wool, and calm temperament, making\n",
       "it valuable for breeding and fiber production.\n",
       "\n",
       "**3. Identify the key differences:**\n",
       "\n",
       "- **Fur:** Llama has thick fur, Vicuna has soft fleece, Alpaca has dense fleece.\n",
       "- **Temperament:** Llama is docile, Vicuna is gentle, Alpaca is calm.\n",
       "- **Wool:** Vicuna has fine wool, Alpaca has soft wool.\n",
       "- **Size:** Llama has long legs, Vicuna has small stature, Alpaca has medium stature.\n",
       "- **Purpose:** Llama is not primarily bred for fiber production, Vicuna is bred for its\n",
       "exceptional fleece, Alpaca is bred for fiber production and companionship.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "The primary difference between a Llama, Vicuna, and Alpaca lies in their respective\n",
       "characteristics and purposes. Llama has thick fur and a docile temperament, Vicuna has\n",
       "soft fleece and a gentle temperament, while Alpaca has dense fleece, soft wool, and a calm\n",
       "temperament."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 s, sys: 153 ms, total: 25 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
    "         system_prompt=\"Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "You are Gemma, a large language model trained by Google. Write out your reasoning step-by-\n",
       "step to be sure you get the right answers!\n",
       "\n",
       " Write a short email to Sam Altman giving reasons to open source GPT-4\n",
       "model\n",
       "**Subject: Reasons for Open Sourcing GPT-4**\n",
       "\n",
       "**Dear Sam Altman,**\n",
       "\n",
       "I understand the importance of keeping GPT-4 confidential, given its competitive edge and\n",
       "potential for commercialization. However, I believe that opening sourcing GPT-4 would\n",
       "significantly benefit the research and development community, while also promoting\n",
       "transparency and accountability.\n",
       "\n",
       "**Reasons for Open Sourcing:**\n",
       "\n",
       "**1. Accelerating Research:**\n",
       "- Open sourcing GPT-4 would make it accessible to a wide range of researchers, enabling\n",
       "them to experiment with the model, improve its performance, and develop new applications.\n",
       "- This would accelerate the pace of research and foster innovation.\n",
       "\n",
       "**2. Fostering Collaboration:**\n",
       "- Open sourcing GPT-4 would encourage collaboration among researchers, allowing them to\n",
       "share ideas, code, and data.\n",
       "- This would foster a sense of community and accelerate progress.\n",
       "\n",
       "**3. Promoting Transparency:**\n",
       "- Open sourcing would make it easier for experts and the public to scrutinize GPT-4's\n",
       "algorithms and evaluate its fairness and bias.\n",
       "- This would enhance transparency and accountability.\n",
       "\n",
       "**4. Accelerating Deployment:**\n",
       "- Open sourcing GPT-4 would make it easier for developers to integrate the model into\n",
       "their applications and services.\n",
       "- This would accelerate the adoption and utilization of GPT-4.\n",
       "\n",
       "**5. Creating a Legacy:**\n",
       "- Open sourcing GPT-4 would create a lasting legacy for the research community, inspiring\n",
       "future generations of engineers and scientists.\n",
       "- It would also promote the development of new tools and resources for natural language\n",
       "processing.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "While I understand the need for confidentiality, I believe that the benefits of open\n",
       "sourcing GPT-4 outweigh the potential risks. By fostering collaboration, accelerating\n",
       "research, promoting transparency, and fostering a legacy, open sourcing would advance the\n",
       "field of natural language processing and benefit society as a whole.\n",
       "\n",
       "**Sincerely,**\n",
       "**Gemma**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.7 s, sys: 211 ms, total: 26.9 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "You are a genius python coder, please think carefully and write the following code:\n",
       "\n",
       " ```python\n",
       "def print_prime(n):\n",
       "   \"\"\"\n",
       "   Print all primes between 1 and n\n",
       "   \"\"\"\n",
       "model\n",
       "```python\n",
       "import math\n",
       "\n",
       "def print_prime(n):\n",
       "   \"\"\"\n",
       "   Print all primes between 1 and n\n",
       "   \"\"\"\n",
       "\n",
       "   Primes = []\n",
       "   for num in range(1, int(n**0.5) + 1):\n",
       "       is_prime = True\n",
       "       for p in primes:\n",
       "           if num % p == 0:\n",
       "               is_prime = False\n",
       "       if is_prime and num not in primes:\n",
       "           Primes.append(num)\n",
       "\n",
       "   print(Primes)\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* The function `print_prime` takes an integer `n` as input.\n",
       "* It uses a list called `Primes` to store all prime numbers.\n",
       "* It iterates over the range from 1 to the square root of `n`.\n",
       "* For each number `num`, it checks if it is prime by checking if it is divisible by any\n",
       "number in the `Primes` list.\n",
       "* If `num` is prime and it is not already in the `Primes` list, it is added to the list.\n",
       "* Finally, the `Primes` list is printed.\n",
       "\n",
       "**Example Usage:**\n",
       "\n",
       "```python\n",
       "print_prime(100)\n",
       "```\n",
       "\n",
       "**Output:**\n",
       "\n",
       "```\n",
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 33, 37, 41, 43, 47, 51, 53, 59, 61, 63, 65, 67,\n",
       "69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n",
       "```\n",
       "\n",
       "**Note:**\n",
       "\n",
       "* This algorithm is efficient for small values of `n`, but it can be slow for large\n",
       "values.\n",
       "* The time complexity of the algorithm is O(n) where `n` is the upper bound of the range.\n",
       "* The space complexity is O(n) as well, since the algorithm uses a list of size n to store\n",
       "the primes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate('''```python\n",
    "def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', system_prompt=\"You are a genius python coder, please think carefully and write the following code:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "user\n",
       "You are Gemma, a large language model trained by Google. Write out your reasoning step-by-\n",
       "step to be sure you get the right answers!\n",
       "\n",
       " Answer the following question by reasoning step by step. The cafeteria had 23 apples. If\n",
       "they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
       "model\n",
       "**Step 1: Subtract 20 from 23.**\n",
       "\n",
       "23 - 20 = 3\n",
       "\n",
       "**Step 2: Add 6 to the remaining 3.**\n",
       "\n",
       "3 + 6 = 9\n",
       "\n",
       "Therefore, the cafeteria has a total of 9 apples left."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
    "         system_prompt=\"You are Gemma, a large language model trained by Google. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

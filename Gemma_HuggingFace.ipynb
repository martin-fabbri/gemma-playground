{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install bitsandbytes accelerate xformers einops\n",
    "!pip -q install hf_transfer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma - 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc27d05df56f432cbc3e7e43f09e6010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", # 'google/gemma-2b-it\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             device_map=\"auto\"\n",
    "                                             )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nWhat is the difference between LLaMAs, Alpacas, and Vicunas<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"What is the difference between LLaMAs, Alpacas, and Vicunas\" },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs.to(model.device),\n",
    "                         max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gemma - 7B 4Bit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "user\n",
       "What is the difference between LLaMAs, Alpacas, and Vicunas\n",
       "model\n",
       "Sure, here is the difference between LLamas, Alpacas, and Vicunas:\n",
       "\n",
       "**LLamas:**\n",
       "\n",
       "* **Origin:** North America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** Gray, brown, black, white\n",
       "* **Temperament:** Friendly, calm, and easy to train\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "**Alpacas:**\n",
       "\n",
       "* **Origin:** South America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** White, fawn, gray, brown\n",
       "* **Temperament:** Friendly, calm, and intelligent\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "**Vicunas:**\n",
       "\n",
       "* **Origin:** South America\n",
       "* **Coat:** Long, dense, soft, and warm\n",
       "* **Color:** White, fawn, gray, brown\n",
       "* **Temperament:** Friendly, calm, and intelligent\n",
       "* **Uses:** Wool production, meat production, and companionship\n",
       "\n",
       "The main differences between LLamas, Alpacas, and Vicunas are their origin, coat color, temperament, and uses. LLamas are North American animals, while Alpacas and Vicunas are South American animals. LLamas have a wide range of coat colors, while Alpacas and Vicunas have a more limited range of coat colors. LLamas are generally friendly and easy to train, while Alpacas and Vicunas are also friendly and easy to train. LLamas are primarily used for wool production and meat production, while Alpacas and Vicunas are primarily used for wool production and meat production."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"## Gemma - 7B 4Bit\"))\n",
    "\n",
    "text = tokenizer.decode(outputs[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# text = text.replace(prompt, '', 1)\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
